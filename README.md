This repository contains practise notebooks on different types of advanced optimization methods and introduction to Tensorflow programming framework.
Optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam are used to discuss the model learning and accuracy. 
Additionally, examples on the use of random minibatches to accelerate convergence and improve optimization are shown. 
The benefits of learning rate decay and how to apply it to different optimization methods is discussed. 
